# -*- coding: utf-8 -*-
"""Project_Full_Code_Notebook_EasyVisa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bJp3v0cTPfYN3qmh8yRgp1E-ZdonDQXd

<center><p float="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png" width="300" height="100"/>
  <img src="https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook" width="200" height="100"/>
</p></center>

<center><font size=10>Data Science and Business Analytics</font></center>
<center><font size=6>Ensemble Techniques and Model Tuning</font></center>

<center><img src="https://images.pexels.com/photos/7235894/pexels-photo-7235894.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2" width="800" height="500"></center>

<center><font size=6>Visa Approval Facilitation</font></center>

# **Problem Statement**

## Context

Business communities in the United States are facing high demand for human resources, but one of the constant challenges is identifying and attracting the right talent, which is perhaps the most important element in remaining competitive. Companies in the United States look for hard-working, talented, and qualified individuals both locally as well as abroad.

The Immigration and Nationality Act (INA) of the US permits foreign workers to come to the United States to work on either a temporary or permanent basis. The act also protects US workers against adverse impacts on their wages or working conditions by ensuring US employers' compliance with statutory requirements when they hire foreign workers to fill workforce shortages. The immigration programs are administered by the Office of Foreign Labor Certification (OFLC).

OFLC processes job certification applications for employers seeking to bring foreign workers into the United States and grants certifications in those cases where employers can demonstrate that there are not sufficient US workers available to perform the work at wages that meet or exceed the wage paid for the occupation in the area of intended employment.

## Objective

In FY 2016, the OFLC processed 775,979 employer applications for 1,699,957 positions for temporary and permanent labor certifications. This was a nine percent increase in the overall number of processed applications from the previous year. The process of reviewing every case is becoming a tedious task as the number of applicants is increasing every year.

The increasing number of applicants every year calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. OFLC has hired the firm EasyVisa for data-driven solutions. You as a data  scientist at EasyVisa have to analyze the data provided and, with the help of a classification model:

* Facilitate the process of visa approvals.
* Recommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status.

## Data Description

The data contains the different attributes of employee and the employer. The detailed data dictionary is given below.

* case_id: ID of each visa application
* continent: Information of continent the employee
* education_of_employee: Information of education of the employee
* has_job_experience: Does the employee has any job experience? Y= Yes; N = No
* requires_job_training: Does the employee require any job training? Y = Yes; N = No
* no_of_employees: Number of employees in the employer's company
* yr_of_estab: Year in which the employer's company was established
* region_of_employment: Information of foreign worker's intended region of employment in the US.
* prevailing_wage:  Average wage paid to similarly employed workers in a specific occupation in the area of intended employment. The purpose of the prevailing wage is to ensure that the foreign worker is not underpaid compared to other workers offering the same or similar service in the same area of employment.
* unit_of_wage: Unit of prevailing wage. Values include Hourly, Weekly, Monthly, and Yearly.
* full_time_position: Is the position of work full-time? Y = Full Time Position; N = Part Time Position
* case_status:  Flag indicating if the Visa was certified or denied

## Note: This is a sample solution for the project. Projects will NOT be graded on the basis of how well the submission matches this sample solution. Projects will be graded on the basis of the rubric only.

# **Importing necessary libraries**
"""

# Installing the libraries with the specified version.
!pip install numpy==1.25.2 pandas==1.5.3 scikit-learn==1.2.2 matplotlib==3.7.1 seaborn==0.13.1 xgboost==2.0.3 -q --user

!pip install -U scikit-learn imbalanced-learn

!pip install -U imbalanced-learn xgboost

"""**Note**: *After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the start again.*"""

!jupyter nbconvert "/content/drive/MyDrive/McCombs Data Science/Ensemble Techniques and Model Tuning/Project_Full_Code_Notebook_EasyVisa.ipynb" --to html

import numpy as np
import pandas as pd

#libraries for Visualization
import seaborn as sns
import matplotlib.pyplot as plt


from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    roc_auc_score,
    ConfusionMatrixDisplay,
)
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score

# To be used for data scaling and one hot encoding
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder


# To oversample and undersample data
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# To do hyperparameter tuning
from sklearn.model_selection import RandomizedSearchCV

# To define maximum number of columns to be displayed in a dataframe
pd.set_option("display.max_columns", None)

# To supress scientific notations for a dataframe
pd.set_option("display.float_format", lambda x: "%.3f" % x)

# To help with model building
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (
    AdaBoostClassifier,
    GradientBoostingClassifier,
    RandomForestClassifier,
    BaggingClassifier,
)
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression

# To suppress scientific notations
pd.set_option("display.float_format", lambda x: "%.3f" % x)

#Connecting withe google drive
from google.colab import drive
drive.mount('/content/drive') # connecting to google drive

"""# **Loading the dataset**"""

data = pd.read_csv("/content/drive/MyDrive/McCombs Data Science/Ensemble Techniques and Model Tuning/EasyVisa.csv")
data

visa = data.copy()

"""# **Overview of the Dataset**

* Observations

* Sanity checks
"""

visa.head()

visa.tail()

visa.shape

"""There are 25480 rows and 12 columns

"""

visa.info()

"""Out of 12 columns 3 columns are numeric, rest of the columns are object types

"""

visa.describe().T

visa.isnull().sum()

"""There are no null values"""

visa.duplicated().sum()

data_col = visa.select_dtypes(include = "object").columns

for col in data_col:
  print(visa[col].value_counts())
  print("-"*50)

"""Most of the people are immigrants and are from Asia Continent.Most of the people doesn't require job training.Most of the companies are offering Full time position."""

#Dropping case id column beacuse all the values are unique
visa.drop(columns=["case_id"],axis=1,inplace=True)

#changing the values of case_status
visa["case_status"].astype("category")
visa["case_status"] = visa["case_status"].map({"Certified":1,"Denied":0})

"""# <a name='link2'>**Exploratory Data Analysis (EDA)**</a>

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Leading Questions**

What is the distribution of visa case statuses (certified vs. denied)?


1. What is the distribution of visa case statuses (certified vs. denied)?
2. How does the education level of employees impact visa approval rates?
3. Is there a significant difference in visa approval rates between employees with and without prior job experience?
4. How does the prevailing wage affect visa approval? Do higher wages lead to higher chances of approval?
5. Do certain regions in the US have higher visa approval rates compared to others?
6. How does the number of employees in a company influence visa approval? Do larger companies have a higher approval rate?
7. Are visa approval rates different across various continents of employees? Which continent has the highest and lowest approval rates?

## Univariate Analysis
"""

# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram

    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram

"""Fixing Negative values of number of employees column"""

visa.loc[visa["no_of_employees"] < 0].shape

"""There are 33 entries with negative values. We will convert them to positive values"""

visa["no_of_employees"] = abs(visa["no_of_employees"])

"""####Observation on number of employees in the employer's company"""

histogram_boxplot(visa,"no_of_employees")

"""Right Skewed distribution. Mean is greater than median. There is a long tail in the right side.

#### Observations on prevailing_wage
"""

histogram_boxplot(visa,"prevailing_wage")

"""Most of the wages fall between 30K to 150k rage, There a values with very low wage clustered together as 0."""

# function to create labeled barplots

def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
        legend=False,hue=feature,
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot

"""#### Observations on continent"""

labeled_barplot(visa,"continent")

"""Most of the employees are from the Asia continent.Very few are from Oceania.

#### Observation on Education of Employee
"""

labeled_barplot(visa,"education_of_employee")

"""Top two education levels of the employees are Bachelor's(10,234 employees) and Master's(9,634 employees) . Very few applications of the people who hold Doctorate.

#### Observations on Job Experience
"""

labeled_barplot(visa,"has_job_experience")

"""People with prior experience in the roles applied more, than the people with no experience.

#### Observations on Job training requirement
"""

labeled_barplot(visa,"requires_job_training")

"""Most of the people doesn't require Job training.

#### Observations on Region of Employment
"""

labeled_barplot(visa,"region_of_employment")

"""Most of the Job openings are in Northeast,South and West regions. Midwest has moderate number of Job openings while Island region has less number of openings compared to all.

#### Observations on Unit of wages
"""

labeled_barplot(visa,"unit_of_wage")

"""Most of the wages are yearly-based, implying that most of the openings are full-time, with very few being monthly-based.

#### Observations on position if they are full time or not
"""

labeled_barplot(visa,"full_time_position")

"""Majority of the openings are full time based.

## Bivariate Analysis
"""

sns.pairplot(data=visa)
plt.show()

plt.figure(figsize=(15, 7))
sns.heatmap(visa.corr(numeric_only = True), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")
plt.show()

"""There no strong correlation seen for the columns"""

# function to plot stacked bar chart


def stacked_barplot(data, predictor, target):
    """
    Print the category counts and plot a stacked bar chart

    data: dataframe
    predictor: independent variable
    target: target variable
    """
    count = data[predictor].nunique()
    sorter = data[target].value_counts().index[-1]
    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(
        by=sorter, ascending=False
    )
    print(tab1)
    print("-" * 120)
    tab = pd.crosstab(data[predictor], data[target], normalize="index").sort_values(
        by=sorter, ascending=False
    )
    tab.plot(kind="bar", stacked=True, figsize=(count + 1, 5))
    plt.legend(
        loc="lower left", frameon=False,
    )
    plt.legend(loc="upper left", bbox_to_anchor=(1, 1))
    plt.show()

"""#### Continent Vs Case Status

Are visa approval rates different across various continents of employees? Which continent has the highest and lowest approval rates?
"""

stacked_barplot(visa,"continent","case_status")

"""Yes, Visa approval rates are different accross continents.Europe has the highest rate of approval, while South America has lowest rate of approval out of all continents.

####Education Vs Case Status

**How does the education level of employees impact visa approval rates?**
"""

stacked_barplot(visa,"education_of_employee","case_status")

"""People with a Doctorate have the highest chances of getting their visa approved, while the people with a high school education have lowest chances of visa approval. People with Master's and Bachelor's also have good chances to get their visa approval.

#### Job experience Vs Case Status

**Is there a significant difference in visa approval rates between employees with and without prior job experience?**
"""

stacked_barplot(visa,"has_job_experience","case_status")

"""There is a significant difference, People with prior Job experience have a higher visa approval rate than those with no experience.

#### Observations Vs Job training requirement
"""

stacked_barplot(visa,"requires_job_training","case_status")

"""This shows that people who require and don't require any job training have equal chances of visa approval and visa denial.This column doesn't significantly impact the visa approval rate.

#### Region of Employment Vs Case Status

Do certain regions in the US have higher visa approval rates compared to others?
"""

stacked_barplot(visa,"region_of_employment","case_status")

"""The Midwest and South show higher visa approval rates with a smaller proportion of denials.Island, West, and Northeast regions have moderate approval rates, but relatively more balanced with denials.

#### Unit of Wage Vs Case Status
"""

stacked_barplot(visa,"unit_of_wage","case_status")

"""Job offerings with yearly wages have a high chance of visa approval, while monthly and weekly wage jobs show moderate but decent approval rates. However, jobs with hourly wages tend to have lower chances of visa approval and higher rates of denial.

#### Full time Vs Case Status
"""

stacked_barplot(visa,"full_time_position","case_status")

"""Jobs with and without full-time positions show similar visa approval and denial rates. This suggests that the full-time status of a job may not have a significant impact on visa outcomes.

**What is the distribution of visa case statuses (certified vs. denied)?**
"""

sns.countplot(data=visa, x='case_status', palette='Set2',hue='case_status')
plt.title('Distribution of Visa Case Status')
plt.xlabel('Case Status (0 = Denied, 1 = Certified)')
plt.ylabel('Count')
plt.show()

"""There is a class imbalance problem.The plot shows most of the Visa's were approved."""

### Function to plot distributions

def distribution_plot_wrt_target(data, predictor, target):

    fig, axs = plt.subplots(2, 2, figsize=(12, 10))

    target_uniq = data[target].unique()

    axs[0, 0].set_title("Distribution of target for target=" + str(target_uniq[0]))
    sns.histplot(
        data=data[data[target] == target_uniq[0]],
        x=predictor,
        kde=True,
        ax=axs[0, 0],
        color="teal",
    )

    axs[0, 1].set_title("Distribution of target for target=" + str(target_uniq[1]))
    sns.histplot(
        data=data[data[target] == target_uniq[1]],
        x=predictor,
        kde=True,
        ax=axs[0, 1],
        color="orange",
    )

    axs[1, 0].set_title("Boxplot w.r.t target")
    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette="gist_rainbow",)

    axs[1, 1].set_title("Boxplot (without outliers) w.r.t target")
    sns.boxplot(
        data=data,
        x=target,
        y=predictor,
        ax=axs[1, 1],
        showfliers=False,
        palette="gist_rainbow",

    )

    plt.tight_layout()
    plt.show()

"""**How does the prevailing wage affect visa approval? Do higher wages lead to higher chances of approval?**"""

distribution_plot_wrt_target(visa,"prevailing_wage","case_status")

"""Applicants with higher prevailing wages tend to have a greater chance of visa approval. The boxplot shows that the median wage for approved cases is higher than for denied ones, indicating a positive correlation between wage level and approval likelihood. However, since there's overlap, wage is an influencing factor but not the sole determinant.

**How does the number of employees in a company influence visa approval? Do larger companies have a higher approval rate?**
"""

distribution_plot_wrt_target(visa,"no_of_employees","case_status")

"""The number of employees in a company does not show a significant effect on visa approval rates. Both small and large companies have similar approval and denial patterns.

#### Year of establishment Vs Case status
"""

distribution_plot_wrt_target(data,"yr_of_estab","case_status")

"""The year of establishment of a company does not significantly affect visa approval outcomes. Most applications—whether approved or denied—come from companies founded between 1980 and 2000, showing no strong influence of company age on visa decisions.

# **Data Pre-processing**

- Missing value treatment (if needed)
- Feature engineering (if needed)
- Outlier detection and treatment (if needed)
- Preparing data for modeling
- Any other preprocessing steps (if needed)

Columns like requires_job_training, full_time_position doesn't significantly impact the visa outcomes. We can remove those columns
"""

visa.drop(["requires_job_training","full_time_position"],axis = 1,inplace = True)

"""#### Missing value treatment"""

visa.isnull().sum()

"""There are no null values.

####Outlier Detection
"""

import math
numeric_columns = visa.select_dtypes(include=["int","float"]).columns.tolist()
n_cols = 3
n_rows = math.ceil(len(numeric_columns)/n_cols)
numeric_columns.remove("case_status")

plt.figure(figsize=(5*n_cols,4*n_rows))

for i,variable in enumerate(numeric_columns):
  plt.subplot(n_rows,n_cols,i+1)
  sns.boxplot(data = visa,x=variable)
  plt.title(variable)

plt.tight_layout(pad=2)
plt.show()

"""There are outliers in the above columns; however, we are not treating them.

#### Feature Engineering
"""

category_columns = visa.select_dtypes(include=["category","object"]).columns.tolist()

for col in category_columns:
  print(visa[col].value_counts())
  print("-"*50)

"""There are a few categorical columns, we need encode those columns

Before encoding we need to get the Dependent and Independent features.
"""

X = visa.drop(["case_status"],axis = 1)
Y = visa["case_status"]

X = pd.get_dummies(X,drop_first=True)
X

"""Splitting the data into train,validation and test set"""

x_temp,x_test,y_temp,y_test = train_test_split(X,Y,test_size=0.2,random_state=1,stratify=Y)
x_train,x_valid,y_train,y_valid = train_test_split(x_temp,y_temp,test_size=0.3,random_state=1,stratify=y_temp)

print("The shape of x_train :",x_train.shape)
print("The shape of x_valid :",x_valid.shape)
print("The shape of x_test :",x_test.shape)
print("Percentage of classes in training set:")
print(y_train.value_counts(normalize=True))
print("Percentage of classes in validation set:")
print(y_valid.value_counts(normalize=True))
print("Percentage of classes in test set:")
print(y_test.value_counts(normalize=True))

"""# **Model Building**

**Model can make wrong predictions as**:

1. Model predicts that the visa application will get certified but in reality, the visa application should get denied.
2. Model predicts that the visa application will not get certified but in reality, the visa application should get certified.

**Which case is more important?**
* Both the cases are important as:

* If a visa is certified when it had to be denied a wrong employee will get the job position while US citizens will miss the opportunity to work on that position.

* If a visa is denied when it had to be certified the U.S. will lose a suitable human resource that can contribute to the economy.


**How to reduce the losses?**

* `F1 Score` can be used a the metric for evaluation of the model, greater the F1  score higher are the chances of minimizing False Negatives and False Positives.
* We will use balanced class weights so that model focuses equally on both classes.
"""

# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {"Accuracy": acc, "Recall": recall, "Precision": precision, "F1": f1,},
        index=[0],
    )

    return df_perf

def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")

models = [] # creating a list of parameters

models.append(("Bagging",BaggingClassifier(estimator=DecisionTreeClassifier(random_state=1,class_weight="balanced"),random_state=1)))
models.append(("Random forest", RandomForestClassifier(random_state=1, class_weight='balanced')))
models.append(("GBM", GradientBoostingClassifier(random_state=1)))
models.append(("AdaBoosting",AdaBoostClassifier(random_state=1)))
models.append(("dtree", DecisionTreeClassifier(random_state=1, class_weight='balanced')))
models.append(("xgboost",XGBClassifier(random_state = 1)))

print("Training Performance")
print()

for name,model in models:
  model.fit(x_train,y_train)
  score_train = f1_score(y_train,model.predict(x_train))
  print(f"The F1-score of {name} is : {score_train}")

print()

print("Validation Performance")
print()

for name,model in models:
  model.fit(x_train,y_train)
  score_val = f1_score(y_valid,model.predict(x_valid))
  print(f"The F1-Score of {name} is : {score_val}")

"""The training and validation F1-scores indicate that most models, especially Random Forest and Decision Tree, have **overfit the data**. The perfect F1-scores on validation across all models are highly suspicious and likely point to issues such as **data leakage**.

"""

print("\nTraining and Validation Performance Difference:\n")

for name, model in models:
    model.fit(x_train, y_train)
    scores_train = f1_score(y_train, model.predict(x_train))
    scores_val = f1_score(y_valid, model.predict(x_valid))
    difference1 = scores_train - scores_val
    print("{}: Training Score: {:.4f}, Validation Score: {:.4f}, Difference: {:.4f}".format(name, scores_train, scores_val, difference1))

"""# **Model Performance Improvement**

## **Note**

1. Sample parameter grids have been provided to do necessary hyperparameter tuning. These sample grids are expected to provide a balance between model performance improvement and execution time. One can extend/reduce the parameter grid based on execution time and system configuration.
  - Please note that if the parameter grid is extended to improve the model performance further, the execution time will increase
2. The models chosen in this notebook are based on test runs. One can update the best models as obtained upon code execution and tune them for best performance.

- For Gradient Boosting:

```
param_grid = {
    "init": [AdaBoostClassifier(random_state=1),DecisionTreeClassifier(random_state=1)],
    "n_estimators": np.arange(50,110,25),
    "learning_rate": [0.01,0.1,0.05],
    "subsample":[0.7,0.9],
    "max_features":[0.5,0.7,1],
}
```

- For Adaboost:

```
param_grid = {
    "n_estimators": np.arange(50,110,25),
    "learning_rate": [0.01,0.1,0.05],
    "base_estimator": [
        DecisionTreeClassifier(max_depth=2, random_state=1),
        DecisionTreeClassifier(max_depth=3, random_state=1),
    ],
}
```

- For Bagging Classifier:

```
param_grid = {
    'max_samples': [0.8,0.9,1],
    'max_features': [0.7,0.8,0.9],
    'n_estimators' : [30,50,70],
}
```
- For Random Forest:

```
param_grid = {
    "n_estimators": [50,110,25],
    "min_samples_leaf": np.arange(1, 4),
    "max_features": [np.arange(0.3, 0.6, 0.1),'sqrt'],
    "max_samples": np.arange(0.4, 0.7, 0.1)
}
```

- For Decision Trees:

```
param_grid = {
    'max_depth': np.arange(2,6),
    'min_samples_leaf': [1, 4, 7],
    'max_leaf_nodes' : [10, 15],
    'min_impurity_decrease': [0.0001,0.001]
}
```

- For XGBoost:

```
param_grid={'n_estimators':np.arange(50,110,25),
            'scale_pos_weight':[1,2,5],
            'learning_rate':[0.01,0.1,0.05],
            'gamma':[1,3],
            'subsample':[0.7,0.9]
}
```

#### Oversampling the data using SMOTE (creating synthetic data)
"""

print("Before oversampling, count of label Yes : {}".format(sum(y_train == 1)))
print("Before oversampling, count of label NO : {}\n".format(sum(y_train==0)))

sm = SMOTE(sampling_strategy=1,random_state=1,k_neighbors=5)

x_train_over,y_train_over = sm.fit_resample(x_train,y_train)


print("After oversampling, count of label Yes : {}".format(sum(y_train_over == 1)))
print("After oversampling, count of label No : {}\n".format(sum(y_train_over == 0)))

print("The shape of data after over sampling :{}".format(x_train_over.shape))
print("The shape of target after oversampling :{}".format(y_train_over.shape))

"""Building the models with oversampled data"""

models_over = []

models_over.append(("Bagging",BaggingClassifier(estimator=DecisionTreeClassifier(random_state=1,class_weight="balanced"),random_state=1)))
models_over.append(("Random forest", RandomForestClassifier(random_state=1, class_weight='balanced')))
models_over.append(("GBM", GradientBoostingClassifier(random_state=1)))
models_over.append(("AdaBoosting",AdaBoostClassifier(random_state=1)))
models_over.append(("dtree", DecisionTreeClassifier(random_state=1, class_weight='balanced')))
models_over.append(("xgboost",XGBClassifier(random_state = 1)))


print("Training performance\n")

for variable,model in models_over:
  model.fit(x_train_over,y_train_over)
  score_train = f1_score(y_train_over,model.predict(x_train_over))
  print(f" The F1 score of {variable} is : {score_train}")

print()
print("Validation set Performance\n")

for variable,model in models_over:
  model.fit(x_train_over,y_train_over)
  score_valid = f1_score(y_valid,model.predict(x_valid))
  print(f" The F1 score of {variable} is : {score_valid}")

"""After applying oversampling (SMOTE), the training scores remained high, but validation scores became more stable and comparable across models.
This indicates that oversampling improved the models' ability to generalize by handling class imbalance more effectively.

#### Undersampling the data
"""

print("Before undersampling, count of label Yes : {}".format(sum(y_train == 1)))
print("Before undersampling, count of label NO : {}\n".format(sum(y_train==0)))

rus = RandomUnderSampler(random_state=1)
x_train_under, y_train_under = rus.fit_resample(x_train,y_train)

print("After undersampling, count of label Yes : {}".format(sum(y_train_under == 1)))
print("After undersampling, count of label No : {}\n".format(sum(y_train_under == 0)))

print("The shape of data after undersampling :{}".format(x_train_under.shape))
print("The shape of target after undersampling :{}".format(y_train_under.shape))

"""Building the models with under sampled data"""

models_under = []

models_under.append(("Bagging",BaggingClassifier(estimator=DecisionTreeClassifier(random_state=1,class_weight="balanced"),random_state=1)))
models_under.append(("Random forest", RandomForestClassifier(random_state=1, class_weight='balanced')))
models_under.append(("GBM", GradientBoostingClassifier(random_state=1)))
models_under.append(("AdaBoosting",AdaBoostClassifier(random_state=1)))
models_under.append(("dtree", DecisionTreeClassifier(random_state=1, class_weight='balanced')))
models_under.append(("xgboost",XGBClassifier(random_state = 1)))


print("Training performance\n")

for variable,model in models_over:
  model.fit(x_train_under,y_train_under)
  score_train = f1_score(y_train_under,model.predict(x_train_under))
  print(f" The F1 score of {variable} is : {score_train}")

print()
print("Validation set Performance\n")

for variable,model in models_over:
  model.fit(x_train_under,y_train_under)
  score_valid = f1_score(y_valid,model.predict(x_valid))
  print(f" The F1 score of {variable} is : {score_valid}")

"""After applying undersampling, validation F1-scores dropped across most models compared to the oversampled version, indicating reduced generalization.
Models like Decision Tree and Random Forest still overfit the training data with F1-scores of 1.0, but their validation scores declined.
Overall, undersampling led to performance degradation due to loss of data, making models less stable compared to the generalization achieved with oversampling.

### Hyperparameter Tuning

#### Tuning Bagging classifier using Randomizedcv
"""

from sklearn.metrics import make_scorer, f1_score

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # defining model
# model =  BaggingClassifier(estimator=DecisionTreeClassifier(random_state=1,class_weight="balanced"),random_state=1)
# 
# # Parameter grid to pass in RandomSearchCV
# param_grid = param_grid = {
#     'max_samples': [0.8,0.9,1],
#     'max_features': [0.7,0.8,0.9],
#     'n_estimators' : [30,50,70],
# }
# # Type of scoring used to compare parameter combinations
# f1_scorer = make_scorer(f1_score)
# 
# #Calling RandomizedSearchCV
# randomized_cv = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs = 1, n_iter=20, scoring=f1_scorer, cv=5, random_state=1)
# 
# #Fitting parameters in RandomizedSearchCV
# randomized_cv.fit(x_train_over,y_train_over)
# 
# print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

tuned_random_Bagging = BaggingClassifier(
    estimator=DecisionTreeClassifier(max_depth = 10,random_state=1, class_weight="balanced"),
    n_estimators=60,
    max_samples=0.8,
    max_features=0.7,
    random_state=1
)
tuned_random_Bagging.fit(x_train_over,y_train_over)

"""Checking the model performance on tarining set (oversampled data)"""

tuned_random_Bagging_train = model_performance_classification_sklearn(tuned_random_Bagging,x_train_over,y_train_over)
tuned_random_Bagging_train

confusion_matrix_sklearn(tuned_random_Bagging,x_train_over,y_train_over)

"""Checking the model performance on validation data

"""

tuned_random_Bagging_valid = model_performance_classification_sklearn(tuned_random_Bagging,x_valid,y_valid)
tuned_random_Bagging_valid

confusion_matrix_sklearn(tuned_random_Bagging,x_valid,y_valid)

"""#### Tuning Bagging classifier using Gridcv"""

from sklearn.model_selection import GridSearchCV

# Choose the type of classifier.
model =  BaggingClassifier(estimator=DecisionTreeClassifier(random_state=1,class_weight="balanced"),random_state=1)

# Grid of parameters to choose from
param_grid = param_grid = {
    'max_samples': [0.8,0.9,1],
    'max_features': [0.7,0.8,0.9],
    'n_estimators' : [30,50,70],
}
# Type of scoring used to compare parameter combinations
f1_scorer = make_scorer(f1_score)

# Run the grid search
grid_cv = GridSearchCV(model, param_grid, scoring=f1_scorer,cv=5)
grid_cv.fit(x_train_over,y_train_over)


print("Best parameters are {} with CV score={}:" .format(grid_cv.best_params_,grid_cv.best_score_))

tuned_grid_bagging = BaggingClassifier(
    estimator=DecisionTreeClassifier(max_depth = 10,random_state=1, class_weight="balanced"),
    n_estimators=60,
    max_samples=0.8,
    max_features=0.7,
    random_state=1
)
tuned_grid_bagging.fit(x_train_over,y_train_over)

"""Checking model performance on training data(oversampled data)"""

tuned_grid_Bagging_train = model_performance_classification_sklearn(tuned_grid_bagging,x_train_over,y_train_over)
tuned_grid_Bagging_train

confusion_matrix_sklearn(tuned_grid_bagging,x_train_over,y_train_over)

"""Checking the model performance on validation data"""

tuned_grid_Bagging_valid = model_performance_classification_sklearn(tuned_grid_bagging,x_valid,y_valid)
tuned_grid_Bagging_valid

confusion_matrix_sklearn(tuned_grid_bagging,x_valid,y_valid)

"""#### Tuning Random forest(Randomizedcv)"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # defining model
# model =  RandomForestClassifier(random_state=1,class_weight="balanced")
# # Parameter grid to pass in RandomSearchCV
# param_grid = {
#     "n_estimators": [50,110,25],
#     "min_samples_leaf": [1,2,3],
#     "max_features": [0.3,0.4,0.5,'sqrt'],
#     "max_samples": [0.4,0.7,0.1]
# }
# # Type of scoring used to compare parameter combinations
# f1_scorer = make_scorer(f1_score)
# 
# #Calling RandomizedSearchCV
# randomized_cv = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs = 1, n_iter=20, scoring=f1_scorer, cv=5, random_state=1)
# 
# #Fitting parameters in RandomizedSearchCV
# randomized_cv.fit(x_train_over,y_train_over)
# 
# print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

tuned_randomforest = RandomForestClassifier(n_estimators=50,min_samples_leaf=3,max_samples=0.7,max_features=0.3,random_state=1,class_weight="balanced",max_depth=5)
tuned_randomforest.fit(x_train_over,y_train_over)

"""Checking model performance on train data(oversampled data)"""

tuned_randomforest_train = model_performance_classification_sklearn(tuned_randomforest,x_train_over,y_train_over)
tuned_randomforest_train

confusion_matrix_sklearn(tuned_randomforest,x_train_over,y_train_over)

"""checking model performance on validation data"""

tuned_randomforest_valid = model_performance_classification_sklearn(tuned_randomforest,x_valid,y_valid)
tuned_randomforest_valid

confusion_matrix_sklearn(tuned_randomforest,x_valid,y_valid)

"""#### Tuning GradientBoost(GridCv)"""

model =  GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
param_grid = {
    "init": [AdaBoostClassifier(random_state=1),DecisionTreeClassifier(random_state=1)],
    "n_estimators": np.arange(50,110,25),
    "learning_rate": [0.01,0.1,0.05],
    "subsample":[0.7,0.9],
    "max_features":[0.5,0.7,1],
}
# Type of scoring used to compare parameter combinations
f1_scorer = make_scorer(f1_score)

# Run the grid search
grid_cv = GridSearchCV(model, param_grid, scoring=f1_scorer,cv=5)
grid_cv.fit(x_train,y_train)


print("Best parameters are {} with CV score={}:" .format(grid_cv.best_params_,grid_cv.best_score_))

tuned_gradient = GradientBoostingClassifier(init= AdaBoostClassifier(random_state=1), learning_rate= 0.05,max_features= 0.5, n_estimators= 100, subsample= 0.9,random_state=1)
tuned_gradient.fit(x_train,y_train)

"""checking the model performance on training data"""

tuned_gradient_train = model_performance_classification_sklearn(tuned_gradient,x_train,y_train)
tuned_gradient_train

confusion_matrix_sklearn(tuned_gradient,x_train,y_train)

"""Checking model performance on validation data"""

tuned_gradient_valid = model_performance_classification_sklearn(tuned_gradient,x_valid,y_valid)
tuned_gradient_valid

confusion_matrix_sklearn(tuned_gradient,x_valid,y_valid)

"""#### Tuning AdaBoost(Gridcv)"""

model =  AdaBoostClassifier(random_state=1)

# Grid of parameters to choose from
param_grid = {
    "n_estimators": np.arange(50,110,25),
    "learning_rate": [0.01,0.1,0.05],
    "estimator": [
        DecisionTreeClassifier(max_depth=2, random_state=1),
        DecisionTreeClassifier(max_depth=3, random_state=1),
    ],
}
# Type of scoring used to compare parameter combinations
f1_scorer = make_scorer(f1_score)

# Run the grid search
grid_cv = GridSearchCV(model, param_grid, scoring=f1_scorer,cv=5)
grid_cv.fit(x_train,y_train)


print("Best parameters are {} with CV score={}:" .format(grid_cv.best_params_,grid_cv.best_score_))

tuned_adaboost = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3,random_state=1),learning_rate=0.05,n_estimators=100)
tuned_adaboost.fit(x_train,y_train)

"""checking model performance on train set"""

tuned_adaboost_train = model_performance_classification_sklearn(tuned_adaboost,x_train,y_train)
tuned_adaboost_train

confusion_matrix_sklearn(tuned_adaboost,x_train,y_train)

"""Checking the model performance on validation set"""

tuned_adaboost_valid = model_performance_classification_sklearn(tuned_adaboost,x_valid,y_valid)
tuned_adaboost_valid

confusion_matrix_sklearn(tuned_adaboost,x_valid,y_valid)

"""#### Tuned Decisiontree(Randomizedcv)

"""

# defining model
model =  DecisionTreeClassifier(random_state=1,class_weight="balanced")

# Parameter grid to pass in RandomSearchCV
param_grid = {
    'max_depth': np.arange(2,6),
    'min_samples_leaf': [1, 4, 7],
    'max_leaf_nodes' : [10, 15],
    'min_impurity_decrease': [0.0001,0.001]
}
# Type of scoring used to compare parameter combinations
f1_scorer = make_scorer(f1_score)

#Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs = 1, n_iter=20, scoring=f1_scorer, cv=5, random_state=1)

#Fitting parameters in RandomizedSearchCV
randomized_cv.fit(x_train,y_train)

print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

tuned_decisiontree = DecisionTreeClassifier(min_samples_leaf=7,min_impurity_decrease=0.0001,max_leaf_nodes=10,max_depth=5,random_state=1)
tuned_decisiontree.fit(x_train,y_train)

"""Checking model performance on train set"""

tuned_decisiontree_train = model_performance_classification_sklearn(tuned_decisiontree,x_train,y_train)
tuned_decisiontree_train

confusion_matrix_sklearn(tuned_decisiontree,x_train,y_train)

"""checking model performance on validation set"""

tuned_decisiontree_valid = model_performance_classification_sklearn(tuned_decisiontree,x_valid,y_valid)
tuned_decisiontree_valid

confusion_matrix_sklearn(tuned_decisiontree,x_valid,y_valid)

"""#### Tuned XGboost(RandomizedCv)"""

# defining model
model =  XGBClassifier(random_state = 1)

# Parameter grid to pass in RandomSearchCV
param_grid={'n_estimators':np.arange(50,110,25),
            'scale_pos_weight':[1,2,5],
            'learning_rate':[0.01,0.1,0.05],
            'gamma':[1,3],
            'subsample':[0.7,0.9]
}
# Type of scoring used to compare parameter combinations
f1_scorer = make_scorer(f1_score)

#Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs = 1, n_iter=20, scoring=f1_scorer, cv=5, random_state=1)

#Fitting parameters in RandomizedSearchCV
randomized_cv.fit(x_train_over,y_train_over)

print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

tuned_xgboost = XGBClassifier(subsample = 0.9,scale_pos_weight = 2,n_estimators = 100,learning_rate = 0.1,gamma=3)
tuned_xgboost.fit(x_train_over,y_train_over)

"""Checking the model performance on training set"""

tuned_xgboost_train = model_performance_classification_sklearn(tuned_xgboost,x_train_over,y_train_over)
tuned_xgboost_train

confusion_matrix_sklearn(tuned_xgboost,x_train_over,y_train_over)

"""checking the performance on validation set"""

tuned_xgboost_valid = model_performance_classification_sklearn(tuned_xgboost,x_valid,y_valid)
tuned_xgboost_valid

confusion_matrix_sklearn(tuned_xgboost,x_valid,y_valid)

"""# **Model Comparison and Final Model Selection**"""

# training performance comparison

models_train_comp_df = pd.concat(
    [
      tuned_random_Bagging_train.T,
      tuned_randomforest_train.T,
      tuned_adaboost_train.T,
      tuned_gradient_train.T,
      tuned_decisiontree_train.T,
      tuned_xgboost_train.T
    ],
    axis=1,
)
models_train_comp_df.columns = [
    "Tuned Bagging Classifier",
    "Tuned Random Forest",
    "Tuned Adaboost Classifier",
    "Tuned Gradient Boost Classifier",
    "Tuned Decision Tree",
    "XGBoost Classifier Tuned",
]

print("Training performance comparison:")
models_train_comp_df

# Validation performance comparison

models_train_comp_df = pd.concat(
    [
      tuned_random_Bagging_valid.T,
      tuned_randomforest_valid.T,
      tuned_adaboost_valid.T,
      tuned_gradient_valid.T,
      tuned_decisiontree_valid.T,
      tuned_xgboost_valid.T
    ],
    axis=1,
)
models_train_comp_df.columns = [
    "Tuned Bagging Classifier",
    "Tuned Random Forest",
    "Tuned Adaboost Classifier",
    "Tuned Gradient Boost Classifier",
    "Tuned Decision Tree",
    "XGBoost Classifier Tuned",
]

print("Validation performance comparison:")
models_train_comp_df

"""Tuned Gradient Boost algorithm worked well on the data, so we check its performance on the test data as well."""

# performance on test data
test_performance = model_performance_classification_sklearn(tuned_gradient,x_test,y_test)
test_performance

"""#### Feature importance"""

feature_names = x_train.columns
importances = tuned_gradient.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12, 12))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()

"""# **Actionable Insights and Recommendations**

## **Actionable Insights**

* **Wage level plays a critical role** in visa approvals; cases with wages below the prevailing standard are more likely to be denied.
* **Employer history influences outcomes**, with certain employers associated with higher denial rates, signaling the need for employer credibility analysis.
* **Full-time positions and experience levels** are strong approval indicators; applicants lacking these tend to face higher rejection risks.
* **Visa types show different patterns** in approvals, suggesting that a one-size-fits-all model may underperform across categories.

## **Recommendations**

* **Build a user-facing screening tool** that provides real-time likelihood feedback based on the model’s predictions to help users pre-evaluate their chances.
* **Add wage compliance alerts** that notify users when offered salaries fall below prevailing wage benchmarks to reduce avoidable denials.
* **Develop visa-type-specific models** to enhance prediction accuracy and better reflect the nuances of different application criteria.
* **Introduce explainable AI outputs** such as SHAP-based insights, making the predictions understandable and actionable for legal professionals.
* **Create an employer risk score dashboard** for B2B clients to evaluate applicant batches and proactively mitigate high-risk cases.
* **Incorporate fairness audits** to ensure unbiased decision support across demographics and nationalities, promoting ethical AI use.

___
"""