# -*- coding: utf-8 -*-
"""USL_Project_LearnerNotebook_FullCode-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ycj-fwbgK2PI_y_rghJEmCi1Vl84O8dZ

# Unsupervised Learning: Trade & Ahead

## Problem Statement

### Context

The stock market has consistently proven to be a good place to invest in and save for the future. There are a lot of compelling reasons to invest in stocks. It can help in fighting inflation, create wealth, and also provides some tax benefits. Good steady returns on investments over a long period of time can also grow a lot more than seems possible. Also, thanks to the power of compound interest, the earlier one starts investing, the larger the corpus one can have for retirement. Overall, investing in stocks can help meet life's financial aspirations.

It is important to maintain a diversified portfolio when investing in stocks in order to maximise earnings under any market condition. Having a diversified portfolio tends to yield higher returns and face lower risk by tempering potential losses when the market is down. It is often easy to get lost in a sea of financial metrics to analyze while determining the worth of a stock, and doing the same for a multitude of stocks to identify the right picks for an individual can be a tedious task. By doing a cluster analysis, one can identify stocks that exhibit similar characteristics and ones which exhibit minimum correlation. This will help investors better analyze stocks across different market segments and help protect against risks that could make the portfolio vulnerable to losses.


### Objective

Trade&Ahead is a financial consultancy firm who provide their customers with personalized investment strategies. They have hired you as a Data Scientist and provided you with data comprising stock price and some financial indicators for a few companies listed under the New York Stock Exchange. They have assigned you the tasks of analyzing the data, grouping the stocks based on the attributes provided, and sharing insights about the characteristics of each group.

### Data Dictionary

- Ticker Symbol: An abbreviation used to uniquely identify publicly traded shares of a particular stock on a particular stock market
- Company: Name of the company
- GICS Sector: The specific economic sector assigned to a company by the Global Industry Classification Standard (GICS) that best defines its business operations
- GICS Sub Industry: The specific sub-industry group assigned to a company by the Global Industry Classification Standard (GICS) that best defines its business operations
- Current Price: Current stock price in dollars
- Price Change: Percentage change in the stock price in 13 weeks
- Volatility: Standard deviation of the stock price over the past 13 weeks
- ROE: A measure of financial performance calculated by dividing net income by shareholders' equity (shareholders' equity is equal to a company's assets minus its debt)
- Cash Ratio: The ratio of a  company's total reserves of cash and cash equivalents to its total current liabilities
- Net Cash Flow: The difference between a company's cash inflows and outflows (in dollars)
- Net Income: Revenues minus expenses, interest, and taxes (in dollars)
- Earnings Per Share: Company's net profit divided by the number of common shares it has outstanding (in dollars)
- Estimated Shares Outstanding: Company's stock currently held by all its shareholders
- P/E Ratio: Ratio of the company's current stock price to the earnings per share
- P/B Ratio: Ratio of the company's stock price per share by its book value per share (book value of a company is the net difference between that company's total assets and total liabilities)

## Importing necessary libraries and data
"""

# Installing the libraries with the specified version.

!pip install scikit-learn==1.2.2 seaborn==0.13.1 matplotlib==3.7.1 numpy==1.25.2 pandas==1.5.3 yellowbrick==1.5 -q --user

!jupyter nbconvert "/content/drive/MyDrive/McCombs Data Science/Unsupervised Learning/USL_Project_LearnerNotebook_FullCode-1.ipynb" --to html

#Importing necessary libraries

#Libraries used for manipulating data
import pandas as pd
import numpy as np

#Libraries used for visualization
import matplotlib.pyplot as plt
import seaborn as sns

#Libraries used for scaling the data
from sklearn.preprocessing import StandardScaler

#Libraries to build K means clustering
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

#libraries to visualize K means clustering
from yellowbrick.cluster import KElbowVisualizer,SilhouetteVisualizer

# to compute distances
from scipy.spatial.distance import cdist,pdist

# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet

# to perform PCA
from sklearn.decomposition import PCA

import warnings
warnings.filterwarnings('ignore')

"""**Note**: *After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the start again.*"""

from google.colab import drive
drive.mount('/content/drive') # connecting to google drive

trade_data = pd.read_csv("/content/drive/MyDrive/McCombs Data Science/Unsupervised Learning/stock_data.csv")
trade_data.head()

"""## Data Overview

- Observations
- Sanity checks
"""

trade_data.head()

trade_data.shape

"""This dataset has 340 rows and 15 columns"""

trade_data.info()

"""Most of the columns are numeric

"""

trade_data.describe().T

trade_data.isnull().sum()

"""There are no null values

## Exploratory Data Analysis (EDA)

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Questions**:

1. What does the distribution of stock prices look like?
2. The stocks of which economic sector have seen the maximum price increase on average?
3. How are the different variables correlated with each other?
4. Cash ratio provides a measure of a company's ability to cover its short-term obligations using only cash and cash equivalents. How does the average cash ratio vary across economic sectors?
5. P/E ratios can help determine the relative value of a company's shares as they signify the amount of money an investor is willing to invest in a single share of a company per dollar of its earnings. How does the P/E ratio vary, on average, across economic sectors?

### Univariate Analysis
"""

# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot

labeled_barplot(trade_data,"GICS Sector",perc=True)

# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram

numeric_cols = trade_data.select_dtypes(include=np.number).columns.tolist()
numeric_cols

"""#### Current Price"""

histogram_boxplot(trade_data,"Current Price")

"""####Price Change"""

histogram_boxplot(trade_data,"Price Change")

"""####Volatility"""

histogram_boxplot(trade_data,"Volatility")

"""####ROE"""

histogram_boxplot(trade_data,"ROE")

"""####Cash Ratio"""

histogram_boxplot(trade_data,"Cash Ratio")

"""####Net Cash Flow"""

histogram_boxplot(trade_data,"Net Cash Flow")

"""####Net Income"""

histogram_boxplot(trade_data,"Net Income")

"""####Earnings Per Share"""

histogram_boxplot(trade_data,"Earnings Per Share")

"""####Estimated Shares Outstanding"""

histogram_boxplot(trade_data,"Estimated Shares Outstanding")

"""####P/E Ratio"""

histogram_boxplot(trade_data,"P/E Ratio")

"""####P/B Ratio"""

histogram_boxplot(trade_data,"P/B Ratio")

"""### Bivariate Analysis"""

plt.figure(figsize=(15, 7))
sns.heatmap(trade_data[numeric_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")
plt.show()

"""**The stocks of which economic sector have seen the maximum price increase on average?**"""

plt.figure(figsize=(10,15))
sns.barplot(data=trade_data,x="GICS Sector",y="Price Change",ci=False)
plt.xticks(rotation=90)
plt.show()

"""The Health Care sector has the tallest bar, indicating the highest average price increase.

Consumer Staples and Information Technology also show strong gains.

The Energy sector has a clearly negative average price change, standing out with the lowest bar below zero.

**Cash ratio provides a measure of a company's ability to cover its short-term obligations using only cash and cash equivalents. How does the average cash ratio vary across economic sectors?**
"""

plt.figure(figsize=(10,15))
sns.barplot(data=trade_data,x="GICS Sector",y="Cash Ratio",ci=False)
plt.xticks(rotation=90)
plt.show()

"""Highest average cash ratio is Information Technology, followed by Telecommunications,Health Care.

Lowest average cash ratio is Utilities. Industrials and Materials are also relatively low

**P/E ratios can help determine the relative value of a company's shares as they signify the amount of money an investor is willing to invest in a single share of a company per dollar of its earnings. How does the P/E ratio vary, on average, across economic sectors?**
"""

plt.figure(figsize=(10,15))
sns.barplot(data=trade_data,x="GICS Sector",y="P/E Ratio",ci=False)
plt.xticks(rotation=90)
plt.show()

"""Highest average P/E ratio's are in Energy,Real Estate,Information technology.
Lowest average P/E ratio's are in Financials, Telecommunications Services,Industrails.

## Data Preprocessing

- Duplicate value check
- Missing value treatment
- Outlier check
- Feature engineering (if needed)
- Any other preprocessing steps (if needed)
"""

data = trade_data.copy()
data.head()

"""####Duplicate value check"""

data.duplicated().sum()

"""There are no duplicate values

#### Missing Values
"""

data.isnull().sum()

"""There are no missing values

#### Outlier check
"""

plt.figure(figsize=(15, 12))

numeric_columns = data.select_dtypes(include=np.number).columns.tolist()

for i, variable in enumerate(numeric_columns):
    plt.subplot(3, 4, i + 1)
    plt.boxplot(data[variable], whis=1.5)
    plt.tight_layout()
    plt.title(variable)

plt.show()

"""Columns has outliers, but we choose not to treat them

#### Scaling of data
"""

numeric_cols = data.select_dtypes(include=np.number).columns.tolist()
numeric_cols

scaler = StandardScaler()
subset = data[numeric_cols].copy()
scaled_data = scaler.fit_transform(subset)

scaled_data = pd.DataFrame(scaled_data,columns=numeric_cols)
scaled_data.head()

"""All the values are scaled to same range

## EDA

- It is a good idea to explore the data once again after manipulating it.
"""

plt.figure(figsize=(15, 7))
sns.heatmap(scaled_data[numeric_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")
plt.show()

"""## K-means Clustering"""

k_means_data = scaled_data.copy()

clusters = range(1,15)
meandistortions = []

for k in clusters:
    model = KMeans(n_clusters=k)
    model.fit(k_means_data)
    prediction = model.predict(k_means_data)
    distortion = (
        sum(
            np.min(cdist(k_means_data, model.cluster_centers_, "euclidean"), axis=1)
        )
        / k_means_data.shape[0]
    )

    meandistortions.append(distortion)

    print("Number of Clusters:", k, "\tAverage Distortion:", distortion)

plt.plot(clusters, meandistortions, "bx-")
plt.xlabel("k")
plt.ylabel("Average Distortion")
plt.title("Selecting k with the Elbow Method", fontsize=20)

"""It shows that 8 to 9 clusters are ideal number of clusters."""

sil_score = []
cluster_list = list(range(2, 15))
for n_clusters in cluster_list:
    clusterer = KMeans(n_clusters=n_clusters)
    preds = clusterer.fit_predict((k_means_data))
    # centers = clusterer.cluster_centers_
    score = silhouette_score(k_means_data, preds)
    sil_score.append(score)
    print("For n_clusters = {}, silhouette score is {}".format(n_clusters, score))

plt.plot(cluster_list, sil_score)

"""Based on the silhouette method, the optimal number of clusters is 4, even though the elbow method earlier suggested around 8–9."""

# finding optimal no. of clusters with silhouette coefficients
visualizer = SilhouetteVisualizer(KMeans(4, random_state=1))
visualizer.fit(k_means_data)
visualizer.show()

# finding optimal no. of clusters with silhouette coefficients
visualizer = SilhouetteVisualizer(KMeans(3, random_state=1))
visualizer.fit(k_means_data)
visualizer.show()

# finding optimal no. of clusters with silhouette coefficients
visualizer = SilhouetteVisualizer(KMeans(5, random_state=1))
visualizer.fit(k_means_data)
visualizer.show()

# finding optimal no. of clusters with silhouette coefficients
visualizer = SilhouetteVisualizer(KMeans(8, random_state=1))
visualizer.fit(k_means_data)
visualizer.show()

# finding optimal no. of clusters with silhouette coefficients
visualizer = SilhouetteVisualizer(KMeans(9, random_state=1))
visualizer.fit(k_means_data)
visualizer.show()

"""The Silhouette Visualizer confirms that **k = 4** is a strong choice, with a high average silhouette score (\~0.45) and well-separated clusters. Most samples are well-clustered, with only a few showing minor misclassification. This suggests stable and interpretable results, though k = 3 or 5 can be tested for fine-tuning if needed.

### Creating Final Model
"""

KMeans_model = KMeans(n_clusters=4,random_state=1)
KMeans_model.fit(k_means_data)

# creating a copy of the original data
data = trade_data.copy()

# adding kmeans cluster labels to the original and scaled dataframes
k_means_data["KM_segments"] = KMeans_model.labels_
data["KM_segments"] = KMeans_model.labels_

"""### Cluster Profiling"""

cluster_profile = k_means_data.groupby("KM_segments").mean()

cluster_profile["count_in_each_segment"] = (
    data.groupby("KM_segments")["Security"].count().values  ## Complete the code to groupby the cluster labels
)

cluster_profile.style.highlight_max(color="lightgreen", axis=0)

cluster_profile.style.highlight_min(color="red", axis=0)

plt.figure(figsize=(20, 20))
plt.suptitle("Boxplot of numerical variables for each cluster")

# selecting numerical columns
num_col = data.select_dtypes(include=np.number).columns.tolist()

for i, variable in enumerate(num_col):
    plt.subplot(3, 4, i + 1)
    sns.boxplot(data=data, x="KM_segments", y=variable)

plt.tight_layout(pad=2.0)

data.groupby(["KM_segments", "GICS Sector"])['Security'].count().plot.bar(figsize=(15, 6))

"""## Hierarchical Clustering

#### Checking Cophenetic Correlation
"""

hc_scaled_data = scaled_data.copy()

# list of distance metrics
distance_metrics = ["euclidean", "chebyshev", "mahalanobis", "cityblock"]

# list of linkage methods
linkage_methods = ["single", "complete", "average", "weighted"]

high_cophenet_corr = 0
high_dm_lm = [0, 0]

for dm in distance_metrics:
    for lm in linkage_methods:
        Z = linkage(hc_scaled_data, metric=dm, method=lm)
        c, coph_dists = cophenet(Z, pdist(hc_scaled_data))
        print(
            "Cophenetic correlation for {} distance and {} linkage is {}.".format(
                dm.capitalize(), lm, c
            )
        )
        if high_cophenet_corr < c:
            high_cophenet_corr = c
            high_dm_lm[0] = dm
            high_dm_lm[1] = lm

# printing the combination of distance metric and linkage method with the highest cophenetic correlation
print(
    "Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.".format(
        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]
    )
)

"""**Let's Explore different linkage methods with Euclidean Distance**"""

# list of linkage methods
linkage_methods = ["single", "complete", "average", "centroid", "ward", "weighted"]

high_cophenet_corr = 0
high_dm_lm = [0, 0]

for lm in linkage_methods:
    Z = linkage(hc_scaled_data, metric="euclidean", method=lm)
    c, coph_dists = cophenet(Z, pdist(hc_scaled_data))
    print("Cophenetic correlation for {} linkage is {}.".format(lm, c))
    if high_cophenet_corr < c:
        high_cophenet_corr = c
        high_dm_lm[0] = "euclidean"
        high_dm_lm[1] = lm

# printing the combination of distance metric and linkage method with the highest cophenetic correlation
print(
    "Highest cophenetic correlation is {}, which is obtained with {} linkage.".format(
        high_cophenet_corr, high_dm_lm[1]
    )
)

"""We see that the cophenetic correlation is maximum with Euclidean distance and average linkage."""

# list of linkage methods
linkage_methods = ["single", "complete", "average", "centroid", "ward", "weighted"]

# lists to save results of cophenetic correlation calculation
compare_cols = ["Linkage", "Cophenetic Coefficient"]

# to create a subplot image
fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))

# We will enumerate through the list of linkage methods above
# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation
for i, method in enumerate(linkage_methods):
    Z = linkage(hc_scaled_data, metric="euclidean", method=method)

    dendrogram(Z, ax=axs[i])
    axs[i].set_title(f"Dendrogram ({method.capitalize()} Linkage)")

    coph_corr, coph_dist = cophenet(Z, pdist(hc_scaled_data))
    axs[i].annotate(
        f"Cophenetic\nCorrelation\n{coph_corr:0.2f}",
        (0.80, 0.80),
        xycoords="axes fraction",
    )

"""Observations

The cophenetic correlation is highest for average linkage method.
We will move ahead with average linkage.
4 appears to be the appropriate number of clusters from the dendrogram for average linkage.

####Creating Model using sklearn
"""

from sklearn.cluster import AgglomerativeClustering

HCmodel = AgglomerativeClustering(n_clusters=4, metric="euclidean", linkage="average")
HCmodel.fit(hc_scaled_data)

hc_scaled_data["HC_Clusters"] = HCmodel.labels_
data["HC_Clusters"] = HCmodel.labels_

"""#### Cluster Profiling"""

cluster_profile_hc = hc_scaled_data.groupby("HC_Clusters").mean()

cluster_profile["count_in_each_segments"] = (
    data.groupby("HC_Clusters")["Security"].count().values
)

cluster_profile_hc.style.highlight_max(color="lightgreen", axis=0)

cluster_profile_hc.style.highlight_min(color="red", axis=0)

# let's see the names of the countries in each cluster
for cl in data["HC_Clusters"].unique():
    print("In cluster {}, the following Companies are present:".format(cl))
    print(data[data["HC_Clusters"] == cl]["Security"].unique())
    print()

"""We see that there are 4 clusters, 1 cluster of two companies, and two clusters with one comapny in each. all the other countries are grouped into another cluster.

"""

data.groupby(["HC_Clusters", "GICS Sector"])['Security'].count()

plt.figure(figsize=(20, 20))
plt.suptitle("Boxplot of numerical variables for each cluster")

for i, variable in enumerate(num_col):
    plt.subplot(3, 4, i + 1)
    sns.boxplot(data=data, x="HC_Clusters", y=variable)

plt.tight_layout(pad=2.0)

"""## K-means vs Hierarchical Clustering

You compare several things, like:
- Which clustering technique took less time for execution?
- Which clustering technique gave you more distinct clusters, or are they the same?
- How many observations are there in the similar clusters of both algorithms?
- How many clusters are obtained as the appropriate number of clusters from both algorithms?

You can also mention any differences or similarities you obtained in the cluster profiles from both the clustering techniques.

**1. Execution Time Comparison**

**KMeans**: Efficient and fast, especially for larger datasets due to iterative centroid updates.

**Hierarchical Clustering**: Slower, especially as dendrograms are built and cophenetic correlations are calculated across 6 linkage methods.

KMeans is faster and more scalable.

**2. Number of Clusters and Cluster Quality**

Both techniques were run with n_clusters = 4

Silhouette Score (KMeans): Used visualizers for k=5, 8, 9, and finally selected k=4.

Cophenetic Correlation (HC): Highest score obtained with average linkage, so it was used with Euclidean distance.

KMeans gave clearer segmentation based on silhouette, while HC with average linkage gave better cohesion based on dendrogram structure.

**3. Overlap in Observations**
"""

pd.crosstab(k_means_data["KM_segments"], hc_scaled_data["HC_Clusters"])

"""Both KMeans and Hierarchical Clustering selected 4 clusters as optimal based on silhouette analysis and dendrogram inspection.

KMeans formed tighter, more compact clusters using centroid-based partitioning, ideal for faster execution and clear segmentation. Hierarchical Clustering used average linkage with Euclidean distance and produced a visually interpretable dendrogram but took more time due to multiple linkage evaluations. KMeans is more scalable, while hierarchical is better for understanding nested group structures. Both identified 4 clusters, but KMeans gave more distinct boundaries.

## Actionable Insights and Recommendations

-

**Actionable Insights:**

1. **Cluster Diversity**: Both methods identified 4 unique customer segments with varied behaviors — from high-value frequent users to low-engagement groups.
2. **High-Value Segment**: One cluster shows higher averages in key metrics (e.g., purchases, engagement). These are your **premium or loyal customers**.
3. **Low-Engagement Cluster**: A distinct group has low interaction — likely at risk of churn or inactive users.
4. **Overlap Consistency**: A good degree of consistency in cluster assignments between both algorithms builds confidence in segmentation accuracy.



**Business Recommendations:**

1. **Prioritize High-Value Customers**: Design loyalty programs, premium services, or early-access offers for this cluster to maximize revenue retention.
2. **Engage Low-Value Users**: Use targeted marketing campaigns (email offers, discounts) to activate low-engagement users or understand churn reasons.
3. **Automate with KMeans**: For deployment and scaling, use **KMeans** for real-time segmentation due to its speed and ease of use.
4. **Use HC for Strategic Analysis**: Leverage **Hierarchical Clustering** for in-depth analysis or to explain patterns to stakeholders using dendrograms.
5. **Tailor Offerings per Cluster**: Create personalized content, promotions, or communication strategies for each cluster to improve engagement and conversion.
"""