# -*- coding: utf-8 -*-
"""Project_SLC_DSBA_INNHotels_FullCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_MCVKHFg__ZrXcPPt3AYxXxrG0IGubPx

# INN Hotels Project

## Context

A significant number of hotel bookings are called-off due to cancellations or no-shows. The typical reasons for cancellations include change of plans, scheduling conflicts, etc. This is often made easier by the option to do so free of charge or preferably at a low cost which is beneficial to hotel guests but it is a less desirable and possibly revenue-diminishing factor for hotels to deal with. Such losses are particularly high on last-minute cancellations.

The new technologies involving online booking channels have dramatically changed customers’ booking possibilities and behavior. This adds a further dimension to the challenge of how hotels handle cancellations, which are no longer limited to traditional booking and guest characteristics.

The cancellation of bookings impact a hotel on various fronts:
* Loss of resources (revenue) when the hotel cannot resell the room.
* Additional costs of distribution channels by increasing commissions or paying for publicity to help sell these rooms.
* Lowering prices last minute, so the hotel can resell a room, resulting in reducing the profit margin.
* Human resources to make arrangements for the guests.

## Objective
The increasing number of cancellations calls for a Machine Learning based solution that can help in predicting which booking is likely to be canceled. INN Hotels Group has a chain of hotels in Portugal, they are facing problems with the high number of booking cancellations and have reached out to your firm for data-driven solutions. You as a data scientist have to analyze the data provided to find which factors have a high influence on booking cancellations, build a predictive model that can predict which booking is going to be canceled in advance, and help in formulating profitable policies for cancellations and refunds.

## Data Description
The data contains the different attributes of customers' booking details. The detailed data dictionary is given below.


**Data Dictionary**

* Booking_ID: unique identifier of each booking
* no_of_adults: Number of adults
* no_of_children: Number of Children
* no_of_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel
* no_of_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel
* type_of_meal_plan: Type of meal plan booked by the customer:
    * Not Selected – No meal plan selected
    * Meal Plan 1 – Breakfast
    * Meal Plan 2 – Half board (breakfast and one other meal)
    * Meal Plan 3 – Full board (breakfast, lunch, and dinner)
* required_car_parking_space: Does the customer require a car parking space? (0 - No, 1- Yes)
* room_type_reserved: Type of room reserved by the customer. The values are ciphered (encoded) by INN Hotels.
* lead_time: Number of days between the date of booking and the arrival date
* arrival_year: Year of arrival date
* arrival_month: Month of arrival date
* arrival_date: Date of the month
* market_segment_type: Market segment designation.
* repeated_guest: Is the customer a repeated guest? (0 - No, 1- Yes)
* no_of_previous_cancellations: Number of previous bookings that were canceled by the customer prior to the current booking
* no_of_previous_bookings_not_canceled: Number of previous bookings not canceled by the customer prior to the current booking
* avg_price_per_room: Average price per day of the reservation; prices of the rooms are dynamic. (in euros)
* no_of_special_requests: Total number of special requests made by the customer (e.g. high floor, view from the room, etc)
* booking_status: Flag indicating if the booking was canceled or not.

## Importing necessary libraries and data
"""

# Installing the libraries with the specified version.
!pip install pandas==1.5.3 numpy==1.25.2 matplotlib==3.7.1 seaborn==0.13.1 scikit-learn==1.2.2 statsmodels==0.14.1 -q --user

"""**Note**: *After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the start again.*"""

!jupyter nbconvert "/content/drive/MyDrive/McCombs Data Science/Supervised Learning - Classification/Project_SLC_DSBA_INNHotels_FullCode.ipynb" --to html

from google.colab import drive
drive.mount('/content/drive') # connecting to google drive

# Libraries to help with reading and manipulating data
import pandas as pd
import numpy as np

#Libraries that help with the data visualization
import seaborn as sns
import matplotlib.pyplot as plt

#Libraries used to split the data
from sklearn.model_selection import train_test_split

#Libaries for building a model
from statsmodels.tools.tools import add_constant
import statsmodels.api as sm
from sklearn.linear_model import LogisticRegression

#Libraries to get different metric scores
from sklearn.metrics import(
    f1_score,
    accuracy_score,
    precision_score,
    recall_score,
    precision_recall_curve,
    confusion_matrix,
    roc_auc_score,
    roc_curve,
    ConfusionMatrixDisplay,

)

"""## Data Overview

- Observations
- Sanity checks
"""

#Loading the dataset
INNHotesl_df = pd.read_csv("/content/drive/MyDrive/McCombs Data Science/Supervised Learning - Classification/INNHotelsGroup.csv")
INNHotesl_df

#Reading first five rows
INNHotesl_df.head()

# Reading last five rows
INNHotesl_df.tail()

#Number of rows and columns in the dataset
INNHotesl_df.shape

#Information about the dataset
INNHotesl_df.info()

"""A few of the columns are string and a few are numerical. Booking status is the dependent feature."""

# Shows the statistical summary of the data
INNHotesl_df.describe(include="all").T

#checking for duplicate values
INNHotesl_df.duplicated().sum()

"""There are no duplicate values"""

#checking for missing values
INNHotesl_df.isnull().sum()

"""There are no missing values

## Exploratory Data Analysis (EDA)

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Leading Questions**:
1. What are the busiest months in the hotel?
2. Which market segment do most of the guests come from?
3. Hotel rates are dynamic and change according to demand and customer demographics. What are the differences in room prices in different market segments?
4. What percentage of bookings are canceled?
5. Repeating guests are the guests who stay in the hotel often and are important to brand equity. What percentage of repeating guests cancel?
6. Many guests have special requirements when booking a hotel room. Do these requirements affect booking cancellation?

###Univariate Analysis
"""

data = INNHotesl_df.copy()

def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (15,10))
    kde: whether to show the density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a triangle will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram

"""#### Observations on lead time"""

histogram_boxplot(data,"lead_time")

"""The average lead time is approximately 80–90 days, but:




Most bookings are made much closer to the arrival date, as seen in the peak near zero.

The median lead time is lower, indicating typical guests book closer to their stay.

#### Observations on Average price per room
"""

histogram_boxplot(data,"avg_price_per_room")

"""The average (mean) price per room is approximately $100,slightly higher than the median.

#### Observations on Number of previous bookings Cancelled
"""

# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 2, 6))
    else:
        plt.figure(figsize=(n + 2, 6))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        order=data[feature].value_counts().index[:n],
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot

labeled_barplot(data,"no_of_previous_cancellations",perc=True)

"""####Observations on Number of previous booking not cancelled"""

labeled_barplot(data,"no_of_previous_bookings_not_canceled")

"""#### Observations on arrival date

"""

labeled_barplot(data,"arrival_date")

"""####Observation on arrival month

1.What are the busiest months in the hotel?
"""

labeled_barplot(data,"arrival_month")

"""Most of the bookings were done for October month.
Top 3 busiest months are October, September and August.

#### Observations on Number of adults booked the rooms
"""

labeled_barplot(data,"no_of_adults")

"""Most rooms were booked for 2 adults, suggesting that couples or pairs of travelers were the most common guests in this dataset.

#### Observation on Number of childern
"""

labeled_barplot(data,"no_of_children")

"""Most rooms were booked by guests without children.
And even when children were included, it was usually 1 or 2 at most, indicating small family travel or couples without kids being the primary customer base.

#### Observations on Number of weekend nights
"""

labeled_barplot(data,"no_of_weekend_nights")

"""Most of the rooms were not booked for weekend nights

#### Observations on Number of weekday Nights
"""

labeled_barplot(data,"no_of_week_nights")

"""Most guests stayed for short weekday durations, with 2 nights being the most common length of stay.

#### Observations on Meal Plan
"""

labeled_barplot(data,"type_of_meal_plan")

"""Most of the guests selected meal plan 1 i.e Breakfast

#### Observations on Car parking Space
"""

labeled_barplot(data,"required_car_parking_space")

"""Most of the guests doesn't require a parking Space

#### Observations on Reserved room type
"""

labeled_barplot(data,"room_type_reserved")

"""Most of the guests selected Room type - 1

#### Observations on Market Segment Type

Which market segment do most of the guests come from?
"""

labeled_barplot(data,"market_segment_type")

"""Most customers booked rooms through online channels.

#### Observations on Number of Special Requests
"""

labeled_barplot(data,"no_of_special_requests")

"""Most of the guests made no requests, even if they did, up to 2 requests was most common.

#### Observations on repeated Guests
"""

labeled_barplot(data,"repeated_guest")

"""The vast majority of guests were first-time visitors, and very few were repeat customers.

**What percentage of bookings are Canceled?**

#### Observations on Booking Status
"""

labeled_barplot(data,"booking_status",perc=True)

"""67.2% of bookings are Canceled

### Bivariate Analysis
"""

def stacked_barplot(data, predictor, target):
    """
    Print the category counts and plot a stacked bar chart

    data: dataframe
    predictor: independent variable
    target: target variable
    """
    count = data[predictor].nunique()
    sorter = data[target].value_counts().index[-1]
    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(
        by=sorter, ascending=False
    )
    print(tab1)
    print("-" * 120)
    tab = pd.crosstab(data[predictor], data[target], normalize="index").sort_values(
        by=sorter, ascending=False
    )
    tab.plot(kind="bar", stacked=True, figsize=(count + 5, 5))
    plt.legend(
        loc="lower left", frameon=False,
    )
    plt.legend(loc="upper left", bbox_to_anchor=(1, 1))
    plt.show()

"""####Heat map"""

col_list = data.select_dtypes(include=np.number).columns.tolist()
plt.figure(figsize=(12, 7))
sns.heatmap(
    data[col_list].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()

"""#### Previous Booking Canceled"""

plt.figure(figsize=(8, 5))
sns.barplot(x='repeated_guest', y='no_of_previous_cancellations', data=data)
plt.title('Previous Cancellations vs Repeated Guest')
plt.xlabel('Repeated Guest (0 = No, 1 = Yes)')
plt.ylabel('Number of Previous Cancellations')
plt.show()

"""Repeated guests tend to have more previous cancellations, it means previous bookings were canceled by the repeated customer prior to the current booking.

#### Previous Booking not Cancelled Vs Repeated guests
"""

plt.figure(figsize=(8,5))
plt.title("Previous Bookings not Cancelled Vs Repeated guests")
sns.barplot(x="repeated_guest",y="no_of_previous_bookings_not_canceled",data=data)
plt.xlabel('Repeated Guest (0 = No, 1 = Yes)')
plt.ylabel('Number of Previous Not Canceled')
plt.show()

"""Many repeated guests have previous bookings that were not canceled, confirming they have a history of successful stays — not just cancellations.

**Hotel rates are dynamic and change according to demand and customer demographics. What are the differences in room prices in different market segments?**
"""

plt.figure(figsize=(8,5))
sns.boxplot(x="market_segment_type",y="avg_price_per_room",data=data)
plt.show()

"""Rooms booked through the Online segment tend to have a higher average price compared to those booked Offline or through Corporate channels.

**Repeating guests are the guests who stay in the hotel often and are important to brand equity. Let's see what percentage of repeating guests cancel?**
"""

stacked_barplot(data,"repeated_guest","booking_status")

"""Repeated guests tend to not Cancel the Bookings

**Many guests have special requirements when booking a hotel room. Do these requirements affect booking cancellation?**
"""

stacked_barplot(data,"no_of_special_requests","booking_status")

"""Guests who make special requests are far less likely to cancel their bookings.

####Average Price per room Vs Number of Adults
"""

plt.figure(figsize=(10,5))
sns.barplot(x="no_of_adults",y="avg_price_per_room",data=data)
plt.title('Mean Average Price per Room vs Number of Adults')
plt.xlabel('Number of Adults')
plt.ylabel('Average Price per Room')
plt.show()

"""Rooms booked for more adults tend to cost more

#### Average Price per room Vs Number of Children
"""

plt.figure(figsize=(10,5))
sns.barplot(x="no_of_children",y="avg_price_per_room",data=data)
plt.title('Mean Average Price per Room vs Number of children')
plt.xlabel('Number of children')
plt.ylabel('Average Price per Room')
plt.show()

"""Rooms with up to 2 children tend to cost more.

## Data Preprocessing

- Missing value treatment (if needed)
- Feature engineering (if needed)
- Outlier detection and treatment (if needed)
- Preparing data for modeling
- Any other preprocessing steps (if needed)

#### Missing Value Treatment
"""

INNHotesl_df.isnull().sum()

"""There are no missing values

#### Outlier Detection
"""

import math
numeric_col = data.select_dtypes(include=np.number).columns.tolist()
n_cols = 3
n_rows = math.ceil(len(numeric_col)/n_cols)
plt.figure(figsize=(5*n_cols,4*n_rows))

for i,variable in enumerate(numeric_col):
  plt.subplot(n_rows,n_cols,i+1)
  sns.boxplot(data=data,x=variable)
  plt.title(variable)

plt.tight_layout(pad=2)
plt.show()

"""#### Feature Engineering

##### One Hot encoding
"""

# One hot encoding for categorical Variables
data = INNHotesl_df.copy()
data = pd.get_dummies(data,columns=["type_of_meal_plan","room_type_reserved","market_segment_type"],drop_first=True)
data.head()

"""##### Binary encoding of a target variable"""

#Binary encoding for target variable
data["booking_status"] = data["booking_status"].map({"Not_Canceled" : 0, "Canceled": 1})
data.head()

# Since the output of one hot encoding resulted in True, False values, we convert all the columns to float so that every column will be on the same scale
bool_cols = data.select_dtypes('bool').columns
data[bool_cols] = data[bool_cols].astype(float)
data.head()

"""## EDA

- It is a good idea to explore the data once again after manipulating it.
"""

col_list = data.select_dtypes(include=np.number).columns.tolist()
plt.figure(figsize=(25, 25))
sns.heatmap(
    data[col_list].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()

"""## Data Preperation For Modelling"""

data = data.drop(columns = ["Booking_ID"])
#Getting dependent and Independent features
X = data.drop(["booking_status"],axis=1)
Y = data["booking_status"]

#Adding constant
X=sm.add_constant(X)

#splitting data
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.30,stratify=Y,random_state =1)

print("Shape of Training set : ", X_train.shape)
print("Shape of test set : ", X_test.shape)
print("Percentage of classes in training set:")
print(Y_train.value_counts(normalize=True))
print("Percentage of classes in test set:")
print(Y_test.value_counts(normalize=True))

"""## Building a Logistic Regression model

We will now perform logistic regression using statsmodels, a Python module that provides functions for the estimation of many statistical models, as well as for conducting statistical tests, and statistical data exploration.

Using statsmodels, we will be able to check the statistical validity of our model - identify the significant predictors from p-values that we get for each predictor variable.
"""

logit = sm.Logit(Y_train,X_train.astype(float))
lg = logit.fit(disp=False)

print(lg.summary())

"""### Obervations
- Negative values of the coefficient show that the probability of a person  canceling the booking decreases with the increase of the corresponding attribute value.

- Positive values of the coefficient show that the probability of a person canceling the booking increases with the increase of the corresponding attribute value.

- p-value of a variable indicates if the variable is significant or not. If we consider the significance level to be 0.05 (5%), then any variable with a p-value less than 0.05 would be considered significant.

## Model performance evaluation

**Model can make wrong predictions as**:

1. Predicting a person canceling the booking but in reality the doesn't cancel the booking.

2. Predicting a person doesn't cancel the booking but in reality the person cancels the booking.

**Which case is more important?**

* Both the cases are important as:

 * If we predict that a person will cancel the booking but they actually don’t, the hotel may unnecessarily hold back rooms, leading to lost revenue opportunities.

 * If we predict that a person won’t cancel the booking but they actually do, the hotel may face last-minute cancellations, resulting in vacant rooms and revenue loss.

**How to reduce this loss?**

* We need to reduce both False Negatives and False Positives

* `f1_score` should be maximized as the greater the f1_score, the higher the chances of reducing both False Negatives and False Positives and identifying both the classes correctly
  * fi_score is computed as
  $$f1\_score = \frac{2 * Precision * Recall}{Precision + Recall}$$

**First, let's create functions to calculate different metrics and confusion matrix so that we don't have to use the same code repeatedly for each model.**

* The model_performance_classification_statsmodels function will be used to check the model performance of models.
* The confusion_matrix_statsmodels function will be used to plot confusion matrix.
"""

# defining a function to compute different metrics to check performance of a classification model built using statsmodels
def model_performance_classification_statsmodels(
    model, predictors, target, threshold=0.5
):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    threshold: threshold for classifying the observation as class 1
    """

    # checking which probabilities are greater than threshold
    pred_temp = model.predict(predictors) > threshold #if the predicted probability greater than threshold then 1 else 0
    # rounding off the above values to get classes
    pred = np.round(pred_temp)

    acc = accuracy_score(target, pred)  # to compute Accuracy # True labels and predicted labels
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {"Accuracy": acc, "Recall": recall, "Precision": precision, "F1": f1,},
        index=[0],
    )

    return df_perf

# defining a function to plot the confusion_matrix of a classification model


def confusion_matrix_statsmodels(model, predictors, target, threshold=0.5):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    threshold: threshold for classifying the observation as class 1
    """
    y_pred = model.predict(predictors) > threshold
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")

confusion_matrix_statsmodels(lg,X_train,Y_train)

print("Training performance:")
model_performance_classification_statsmodels(lg, X_train, Y_train)

"""**Observations**

- The f1_score of the model is ~0.68 and we will try to maximize it further

- The variables used to build the model might contain multicollinearity, which will affect the p-values

* We will have to remove multicollinearity from the data to get reliable coefficients and p-values

### Detecting and Dealing with Multicollinearity

There are different ways of detecting (or testing for) multicollinearity. One such way is using the Variation Inflation Factor (VIF).

* **Variance  Inflation  factor**:  Variance  inflation  factors  measure  the  inflation  in  the variances of the regression coefficients estimates due to collinearities that exist among the  predictors.  It  is  a  measure  of  how  much  the  variance  of  the  estimated  regression coefficient $\beta_k$ is "inflated" by  the  existence  of  correlation  among  the  predictor variables in the model.

* **General Rule of thumb**:
  - If VIF is 1 then there is no correlation among the $k$th predictor and the remaining predictor variables, and  hence  the variance of $\beta_k$ is not inflated at all
  - If VIF exceeds 5, we say there is moderate multicollinearity
  - If VIF is equal or exceeding 10, it shows signs of high multi-collinearity

* The purpose of the analysis should dictate which threshold to use
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif_series = pd.Series(
    [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])],
    index=X_train.columns,
    dtype=float,
)
print("Series before feature selection: \n\n{}\n".format(vif_series))

"""Dropping market_segment_type_Offline column


"""

X_train2 = X_train.drop("market_segment_type_Offline", axis=1)
vif_series3 = pd.Series(
    [variance_inflation_factor(X_train2.values, i) for i in range(X_train2.shape[1])],
    index=X_train2.columns,
)
print("Series before feature selection: \n\n{}\n".format(vif_series3))

logit2 = sm.Logit(Y_train, X_train2.astype(float))
lg2 = logit2.fit(disp=False)

print("Training performance:")
model_performance_classification_statsmodels(lg2, X_train2, Y_train)

"""No Significant change in Model performance

**Observations:**
1. Dropping market_segment_type_Offline doesn't have a significant impact on the model performance.
2. We can choose any model to proceed to the next steps.
3. Some of the categorical levels of a variable have VIF>5 which can simply be ignored.
"""

print(lg2.summary())

"""#### Removing High P- values

* For other attributes present in the data, the p-values are high only for few dummy variables and since only one (or some) of the categorical levels have a high p-value we will drop them iteratively as sometimes p-values change after dropping a variable. So, we'll not drop all variables at once.

* Instead, we will do the following repeatedly using a loop:
  - Build a model, check the p-values of the variables, and drop the column with the highest p-value.
  - Create a new model without the dropped feature, check the p-values of the variables, and drop the column with the highest p-value.
  - Repeat the above two steps till there are no columns with p-value > 0.05.


Note: The above process can also be done manually by picking one variable at a time that has a high p-value, dropping it, and building a model again. But that might be a little tedious and using a loop will be more efficient.
"""

# initial list of columns
cols = X_train2.columns.tolist()

# setting an initial max p-value
max_p_value = 1

while len(cols) > 0:
    # defining the train set
    X_train_aux = X_train2[cols]

    # fitting the model
    model = sm.Logit(Y_train, X_train_aux).fit(disp=False)

    # getting the p-values and the maximum p-value
    p_values = model.pvalues
    max_p_value = max(p_values)

    # name of the variable with maximum p-value
    feature_with_p_max = p_values.idxmax()

    if max_p_value > 0.05:
        cols.remove(feature_with_p_max)
    else:
        break

selected_features = cols
print(selected_features)

X_train3 = X_train2[selected_features]

logit3 = sm.Logit(Y_train, X_train3.astype(float))
lg3 = logit3.fit(disp=False)

print(lg3.summary())

"""### Coefficient Interpretations

- Negative values of the coefficient show that the probability of a person  canceling the booking decreases with the increase of the corresponding attribute value.

- Positive values of the coefficient show that the probability of a person canceling the booking increases with the increase of the corresponding attribute value.

**Converting coefficients to odds**

* The coefficients ($\beta$s) of the logistic regression model are in terms of $log(odds)$ and to find the odds, we have to take the exponential of the coefficients
* Therefore, **$odds =  exp(\beta)$**
* The percentage change in odds is given as $(exp(\beta) - 1) * 100$
"""

# converting coefficients to odds
odds = np.exp(lg3.params)

# finding the percentage change
perc_change_odds = (np.exp(lg3.params) - 1) * 100

# removing limit from number of columns to display
pd.set_option("display.max_columns", None)

# adding the odds to a dataframe
pd.DataFrame({"Odds": odds, "Change_odd%": perc_change_odds}, index=X_train3.columns).T

"""### Observations

* For each unit increase in the weekend night, the odds of cancellation increase by approximately 16.42%, holding all other variables constant.

* For each unit increase in required_car_parking_space (i.e., if a guest requests a car parking space), the odds of booking cancellation decrease by approximately 79.83%, holding all other variables constant.

* For every unit increase in lead time (i.e., more days between booking and arrival), the odds of cancellation increase by approximately 58.02%, keeping all else constant.

* Repeated guests are 95% less likely to cancel a booking than new guests, all else equal.

* Guests with a history of cancellations are 32.5% more likely to cancel again, per additional past cancellation.

### Checking the performance of the new model
"""

confusion_matrix_statsmodels(lg3, X_train3, Y_train)

log_reg_model_train_perf = model_performance_classification_statsmodels(
    lg3, X_train3, Y_train
)

print("Training performance:")
log_reg_model_train_perf

"""### Test set performance"""

X_test3 = X_test[list(X_train3.columns)]

# creating confusion matrix
confusion_matrix_statsmodels(lg3, X_test3, Y_test)

log_reg_model_train_perf = model_performance_classification_statsmodels(
    lg3, X_test3, Y_test
)

print("Training performance:")
log_reg_model_train_perf

"""- The model is giving a good f1_score of ~0.68 and ~0.67 on the train and test sets respectively
- As the train and test performances are comparable, the model is not overfitting
- Moving forward we will try to improve the performance of the model

#### Model performance improvement

* Let's see if the f1_score can be improved further by changing the model threshold
* First, we will check the ROC curve, compute the area under the ROC curve (ROC-AUC), and then use it to find the optimal threshold
* Next, we will check the Precision-Recall curve to find the right balance between precision and recall as our metric of choice is f1_score

##### ROC Curve and ROC-AUC

* ROC_AUC on training set
"""

logit_roc_auc_train = roc_auc_score(Y_train, lg3.predict(X_train3))
fpr, tpr, thresholds = roc_curve(Y_train, lg3.predict(X_train3))
plt.figure(figsize=(7, 5))
plt.plot(fpr, tpr, label="Logistic Regression (area = %0.2f)" % logit_roc_auc_train)
plt.plot([0, 1], [0, 1], "r--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver operating characteristic")
plt.legend(loc="lower right")
plt.show()

"""Logistic regression performing good on training set

#### Optimal threshold using AUC-ROC curve
"""

fpr, tpr, thresholds = roc_curve(Y_train, lg3.predict(X_train3))

optimal_idx = np.argmax(tpr - fpr)
optimal_threshold_auc_roc = thresholds[optimal_idx]
print(optimal_threshold_auc_roc)

"""#### Checking the model performance on training set"""

# creating confusion matrix
confusion_matrix_statsmodels(
    lg3, X_train3, Y_train, threshold=optimal_threshold_auc_roc
)

# checking model performance for this model
log_reg_model_train_perf_threshold_auc_roc = model_performance_classification_statsmodels(
    lg3, X_train3, Y_train, threshold=optimal_threshold_auc_roc
)
print("Training performance:")
log_reg_model_train_perf_threshold_auc_roc

"""* Precision of model has decreased but the other metrics like F1 score is increased
* The model is still giving a good performance.

#### Checking model performance on test data
"""

logit_roc_auc_train = roc_auc_score(Y_test, lg3.predict(X_test3))
fpr, tpr, thresholds = roc_curve(Y_test, lg3.predict(X_test3))
plt.figure(figsize=(7, 5))
plt.plot(fpr, tpr, label="Logistic Regression (area = %0.2f)" % logit_roc_auc_train)
plt.plot([0, 1], [0, 1], "r--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver operating characteristic")
plt.legend(loc="lower right")
plt.show()

"""#### Checking model performance on test set"""

# creating confusion matrix
confusion_matrix_statsmodels(
    lg3, X_test3, Y_test, threshold=optimal_threshold_auc_roc
)

# checking model performance for this model
log_reg_model_test_perf_threshold_auc_roc = model_performance_classification_statsmodels(
    lg3, X_test3, Y_test, threshold=optimal_threshold_auc_roc
)
print("Test performance:")
log_reg_model_test_perf_threshold_auc_roc

"""#### Precision recall Curve"""

y_scores = lg3.predict(X_train3)
prec, rec, tre = precision_recall_curve(Y_train, y_scores,)


def plot_prec_recall_vs_tresh(precisions, recalls, thresholds):
    plt.plot(thresholds, precisions[:-1], "b--", label="precision")
    plt.plot(thresholds, recalls[:-1], "g--", label="recall")
    plt.xlabel("Threshold")
    plt.legend(loc="upper left")
    plt.ylim([0, 1])


plt.figure(figsize=(10, 7))
plot_prec_recall_vs_tresh(prec, rec, tre)
plt.show()

"""The threshold is 0.48"""

# setting the threshold
optimal_threshold_curve = 0.48

# creating confusion matrix
confusion_matrix_statsmodels(lg3, X_train3, Y_train, threshold=optimal_threshold_curve)

log_reg_model_train_perf_threshold_curve = model_performance_classification_statsmodels(
    lg3, X_train3, Y_train, threshold=optimal_threshold_curve
)
print("Training performance:")
log_reg_model_train_perf_threshold_curve

"""* The model is performing well on the training set
* There's not much improvement in the model performance as the default threshold is 0.50 and here we get 0.69 as the optimal threshold.
"""

# creating confusion matrix
confusion_matrix_statsmodels(lg3, X_test3, Y_test, threshold=optimal_threshold_curve)

log_reg_model_test_perf_threshold_curve = model_performance_classification_statsmodels(
    lg3, X_test3, Y_test, threshold=optimal_threshold_curve
)
print("Test performance:")
log_reg_model_test_perf_threshold_curve

"""## Final Model Summary"""

# training performance comparison

models_train_comp_df = pd.concat(
    [
        log_reg_model_train_perf.T,
        log_reg_model_train_perf_threshold_auc_roc.T,
        log_reg_model_train_perf_threshold_curve.T,
    ],
    axis=1,
)
models_train_comp_df.columns = [
    "Logistic Regression-default Threshold (0.5)",
    "Logistic Regression-0.32 Threshold",
    "Logistic Regression-0.48 Threshold",
]

print("Training performance comparison:")
models_train_comp_df

# testing performance comparison

models_test_comp_df = pd.concat(
    [

        log_reg_model_test_perf_threshold_auc_roc.T,
        log_reg_model_test_perf_threshold_curve.T,
    ],
    axis=1,
)
models_test_comp_df.columns = [
    "Logistic Regression-0.32 Threshold",
    "Logistic Regression-0.48 Threshold",
]

print("Test set performance comparison:")
models_test_comp_df

"""## Building a Decision Tree model

### Data Preperation for modelling
"""

#importing necessary libraries
# To build model for prediction
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# To tune different models
from sklearn.model_selection import GridSearchCV

# To get diferent metric scores
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    make_scorer,
)

data = INNHotesl_df.copy()
#print(data.head())
data = data.drop(columns = ["Booking_ID"])

#Getting dependent and independent features

X = data.drop(["booking_status"],axis=1)
Y = data["booking_status"]


#adding a constant
X  = sm.add_constant(X)

#Getting dummies
X = pd.get_dummies(X,columns=["type_of_meal_plan","room_type_reserved","market_segment_type"],drop_first=True)
#print(X)

#Binary Encoding
Y = Y.map({"Not_Canceled" : 0, "Canceled": 1})
#print(Y)

#changing the dummies to float datatype
bool_cols = X.select_dtypes(include=bool).columns.tolist()
X[bool_cols] = X[bool_cols].astype('float')
X.head()

#train test split
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.30,random_state=1)

print("Shape of Training set : ", X_train.shape)
print("Shape of test set : ", X_test.shape)
print("Percentage of classes in training set:")
print(Y_train.value_counts(normalize=True))
print("Percentage of classes in test set:")
print(Y_test.value_counts(normalize=True))

model0 = DecisionTreeClassifier(random_state=1)
model0.fit(X_train,Y_train)

"""### Model Evaluation"""

# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {"Accuracy": acc, "Recall": recall, "Precision": precision, "F1": f1,},
        index=[0],
    )

    return df_perf

def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")

confusion_matrix_sklearn(model0,X_train,Y_train)

decision_tree_train_performance_without = model_performance_classification_sklearn(model0,X_train,Y_train)
decision_tree_train_performance_without

"""The Accuracy is around 0.99 i.e 99% which shows Overfitting of the data

#### Checking the performance on the test set
"""

confusion_matrix_sklearn(model0,X_test,Y_test)

decision_tree_test_performance_without = model_performance_classification_sklearn(model0,X_test,Y_test)
decision_tree_test_performance_without

"""The model performed poor on the test set data

#### Decision Tree with Class weights

* If the frequency of class A is 10% and the frequency of class B is 90%, then class B will become the dominant class and the decision tree will become biased toward the dominant classes

* In this case, we will set class_weight = "balanced", which will automatically adjust the weights to be inversely proportional to the class frequencies in the input data

* class_weight is a hyperparameter for the decision tree classifier
"""

model_weight = DecisionTreeClassifier(random_state=1,class_weight='balanced')
model_weight.fit(X_train,Y_train)

confusion_matrix_sklearn(model_weight,X_train,Y_train)

decision_tree_train_performance_with_weight = model_performance_classification_sklearn(model_weight,X_train,Y_train)
decision_tree_train_performance_with_weight

confusion_matrix_sklearn(model_weight,X_test,Y_test)

decision_tree_test_performance_with_weight = model_performance_classification_sklearn(model_weight,X_test,Y_test)
decision_tree_test_performance_with_weight

"""There is a huge difference in all the metrics for train and test data, which shows that the model performed well on the train set but not on the test set, which shows Overfitting.

## Do we need to prune the tree?

### Decision Tree(Pre-Pruning)

**Using GridSearch for Hyperparameter tuning of our tree model**

* Hyperparameter tuning is also tricky in the sense that there is no direct way to calculate how a change in the
  hyperparameter value will reduce the loss of your model, so we usually resort to experimentation. i.e we'll use Grid search
* Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters.
* It is an exhaustive search that is performed on a the specific parameter values of a model.
* The parameters of the estimator/model used to apply these methods are optimized by cross-validated grid-search over a parameter grid.
"""

# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
parameters = {
    "class_weight": [None, "balanced"],
    "max_depth": np.arange(2, 7, 2),
    "max_leaf_nodes": [50, 75, 150, 250],
    "min_samples_split": [10, 30, 50, 70],
}

# Type of scoring used to compare parameter combinations
acc_scorer = make_scorer(recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, Y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, Y_train)

confusion_matrix_sklearn(estimator,X_train,Y_train)

decision_tree_train_performance_pre = model_performance_classification_sklearn(estimator,X_train,Y_train)
decision_tree_train_performance_pre

confusion_matrix_sklearn(estimator,X_test,Y_test)

decision_tree_test_performance_pre = model_performance_classification_sklearn(estimator,X_test,Y_test)
decision_tree_test_performance_pre

"""* The model is giving a generalized result now since the recall scores on both the train and test data are coming to be around 0.86 which shows that the model is able to generalize well on unseen data."""

feature_names = list(X_train.columns)
importances = estimator.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(20, 10))
out = tree.plot_tree(
    estimator,
    feature_names=feature_names,
    filled=True,
    fontsize=9,
    node_ids=False,
    class_names=None,
)
# below code will add arrows to the decision tree split if they are missing
for o in out:
    arrow = o.arrow_patch
    if arrow is not None:
        arrow.set_edgecolor("black")
        arrow.set_linewidth(1)
plt.show()

# Text report showing the rules of a decision tree -
print(tree.export_text(estimator, feature_names=feature_names, show_weights=True))

"""**Observations from the pre-pruned tree:**

Using the above extracted decision rules we can make interpretations from the decision tree model like:

- If `lead_time <= 151.50`:
  - If `no_of_special_requests <= 0.5`: the guest is **more likely to cancel**.
  - If `no_of_special_requests > 0.5`: the guest is **less likely to cancel**.

- If `lead_time > 151.50`:
  - Regardless of `avg_price_per_room`, the model predicts the guest is **more likely to cancel**,even though the weight suggests a majority are not canceled. This can happen due to how the decision tree uses impurity (like Gini index) to split.

"""

importances = estimator.feature_importances_
importances

# importance of features in the tree building

importances = estimator.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(8, 8))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()

"""###Decision Tree Post pruning

The `DecisionTreeClassifier` provides parameters such as
``min_samples_leaf`` and ``max_depth`` to prevent a tree from overfiting. Cost
complexity pruning provides another option to control the size of a tree. In
`DecisionTreeClassifier`, this pruning technique is parameterized by the
cost complexity parameter, ``ccp_alpha``. Greater values of ``ccp_alpha``
increase the number of nodes pruned. Here we only show the effect of
``ccp_alpha`` on regularizing the trees and how to choose a ``ccp_alpha``
based on validation scores.

**Total impurity of leaves vs effective alphas of pruned tree**

Minimal cost complexity pruning recursively finds the node with the "weakest
link". The weakest link is characterized by an effective alpha, where the
nodes with the smallest effective alpha are pruned first. To get an idea of
what values of ``ccp_alpha`` could be appropriate, scikit-learn provides
`DecisionTreeClassifier.cost_complexity_pruning_path` that returns the
effective alphas and the corresponding total leaf impurities at each step of
the pruning process. As alpha increases, more of the tree is pruned, which
increases the total impurity of its leaves.
"""

clf = DecisionTreeClassifier(random_state=1, class_weight="balanced")
path = clf.cost_complexity_pruning_path(X_train, Y_train)
ccp_alphas, impurities = abs(path.ccp_alphas), path.impurities

pd.DataFrame(path)

fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(ccp_alphas[:-1], impurities[:-1], marker="o", drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")
plt.show()

"""Next, we train a decision tree using the effective alphas. The last value
in ``ccp_alphas`` is the alpha value that prunes the whole tree,
leaving the tree, ``clfs[-1]``, with one node.
"""

clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(
        random_state=1, ccp_alpha=ccp_alpha, class_weight="balanced"
    )
    clf.fit(X_train, Y_train)
    clfs.append(clf)
print(
    "Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
        clfs[-1].tree_.node_count, ccp_alphas[-1]
    )
)

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1, figsize=(10, 7))
ax[0].plot(ccp_alphas, node_counts, marker="o", drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker="o", drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()

recall_train = []
for clf in clfs:
    pred_train = clf.predict(X_train)
    values_train = recall_score(Y_train, pred_train)
    recall_train.append(values_train)

recall_test = []
for clf in clfs:
    pred_test = clf.predict(X_test)
    values_test = recall_score(Y_test, pred_test)
    recall_test.append(values_test)

train_scores = [clf.score(X_train, Y_train) for clf in clfs]
test_scores = [clf.score(X_test, Y_test) for clf in clfs]

fig, ax = plt.subplots(figsize=(15, 5))
ax.set_xlabel("alpha")
ax.set_ylabel("Recall")
ax.set_title("Recall vs alpha for training and testing sets")
ax.plot(
    ccp_alphas, recall_train, marker="o", label="train", drawstyle="steps-post",
)
ax.plot(ccp_alphas, recall_test, marker="o", label="test", drawstyle="steps-post")
ax.legend()
plt.show()

# creating the model where we get highest train and test recall
index_best_model = np.argmax(recall_test)
best_model = clfs[index_best_model]
print(best_model)

confusion_matrix_sklearn(best_model, X_train, Y_train)

decision_tree_train_performance_post = model_performance_classification_sklearn(best_model, X_train, Y_train)
decision_tree_train_performance_post

confusion_matrix_sklearn(best_model, X_test, Y_test)

decision_tree_test_performance_post= model_performance_classification_sklearn(best_model, X_test, Y_test)
decision_tree_test_performance_post

plt.figure(figsize=(20, 10))

out = tree.plot_tree(
    best_model,
    feature_names=feature_names,
    filled=True,
    fontsize=9,
    node_ids=False,
    class_names=None,
)
for o in out:
    arrow = o.arrow_patch
    if arrow is not None:
        arrow.set_edgecolor("black")
        arrow.set_linewidth(1)
plt.show()

# Text report showing the rules of a decision tree -

print(tree.export_text(best_model, feature_names=feature_names, show_weights=True))

importances = best_model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12, 12))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()

"""## Model Performance Comparison and Conclusions"""

# training performance comparison

models_train_comp_df = pd.concat(
    [
      decision_tree_train_performance_without.T,
      decision_tree_train_performance_with_weight.T,
      decision_tree_train_performance_pre.T,
      decision_tree_train_performance_post.T
    ],
    axis=1,
)
models_train_comp_df.columns = [
    "Decision Tree without class_weight",
    "Decision Tree with class_weight",
    "Decision Tree (Pre-Pruning)",
    "Decision Tree (Post-Pruning)",
]
print("Training performance comparison:")
models_train_comp_df

# testing performance comparison

models_test_comp_df = pd.concat(
    [
        decision_tree_test_performance_without.T,
        decision_tree_test_performance_with_weight.T,
        decision_tree_test_performance_pre.T,
        decision_tree_test_performance_post.T
    ],
    axis=1,
)
models_test_comp_df.columns = [
    "Decision Tree without class_weight",
    "Decision Tree with class_weight",
    "Decision Tree (Pre-Pruning)",
    "Decision Tree (Post-Pruning)",
]
print("Test set performance comparison:")
models_test_comp_df

"""## Actionable Insights and Recommendations

- What profitable policies for cancellations and refunds can the hotel adopt?
- What other recommedations would you suggest to the hotel?

###Insights

1. Guests with **long lead times** and **no special requests** are more likely to cancel.
2. **Repeated guests** and those requesting **car parking** are significantly less likely to cancel.
3. Cancellations are higher in the **online and offline segments**, especially without added preferences.

---

### Recommendations

1. Implement **loyalty programs** and **booking confirmations** for high-risk segments to reduce cancellations.
2. Use the trained model to flag likely cancellations and enable **proactive customer follow-up**.
3. Improve the model further by **adjusting the threshold** or trying **ensemble methods** like Random Forest.
"""